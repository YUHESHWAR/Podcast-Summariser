{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aade3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the transcript file from previous step\n",
    "transcript_path = \"../data/transcriptions/transcript.txt\"\n",
    "\n",
    "# Load transcript lines\n",
    "with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Separate timestamps and text\n",
    "timestamps = []\n",
    "texts = []\n",
    "\n",
    "for line in lines:\n",
    "    if \"]\" in line:\n",
    "        try:\n",
    "            time_part, text = line.strip().split(\"] \")\n",
    "            timestamps.append(time_part[1:])  # Remove leading '['\n",
    "            texts.append(text)\n",
    "        except ValueError:\n",
    "            continue  # skip any malformed lines\n",
    "\n",
    "# Confirm data was loaded\n",
    "print(f\"Loaded {len(texts)} transcript segments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e484e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602ceb8bc6a54b499b725a8e431953b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhes\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuhes\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151a9f3783364b57b38751657713db1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17351ebea97248c681297e78d07d22ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48169654a5942b08dd597286906ebf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32353ecb2026417c92e80751aebc8d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0766b37e59f146c88c878a6b1c805740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032d5724cde249c5bdff369b2c854c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85e931b4e4f497db88873e07520bfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79b24e5a5f243e8886486d40b24a4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6ed75836a04c5e8986dccafa9c0414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07c3554f10743d68f42e0ce301246ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fd8d1c29394537af5807534547dc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated embeddings for 819 segments.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained sentence embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Fast, good balance of speed/quality\n",
    "\n",
    "# Generate embeddings for all transcript segments\n",
    "embeddings = embedder.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Convert to NumPy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings)} segments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clustered transcript into 8 topics.\n"
     ]
    }
   ],
   "source": [
    "# Choose number of clusters (topics) â€” you can tune this\n",
    "num_clusters = 8\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "print(f\"Clustered transcript into {num_clusters} topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffaf0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Topic Cluster 0\n",
      "  [0.16s - 10.92s] We have been a misunderstood and badly mocked org for a long time. Like when we started, we like announced the org at the end of 2015 and\n",
      "  [1112.21s - 1122.77s] Second, we are building in public and we are putting out technology because we think it is important for the world to get access to this early to shape the way it's going to be developed,\n",
      "  [1153.83s - 1164.26s] the technology and shape it with us and provide feedback we believe is really important. The trade off of that is the trade off of building in public, which is we put out things that are going to be deeply\n",
      "\n",
      "ðŸ§  Topic Cluster 1\n",
      "  [1080.61s - 1090.81s] in a sequence of prompts how to understand that it failed to do so previously and where it succeeded. And all of those like multi, like parallel reasonings\n",
      "  [1090.81s - 1101.01s] that it's doing, it just seems like it's struggling. So two separate things going on here. Number one, some of the things that seem like they should be obvious and easy these models\n",
      "  [1101.01s - 1111.33s] really struggle with. So I haven't seen this particular example, but counting characters, counting words, that sort of stuff that is hard for these models to do. Well the way they're architected, that won't be very accurate.\n",
      "\n",
      "ðŸ§  Topic Cluster 2\n",
      "  [1037.48s - 1048.04s] think that was 3.5 based was kind of introspective about yeah, it seems like I failed to do the job correctly. And\n",
      "  [1281.35s - 1291.63s] the COVID virus leak from a lab? Again, answer, very nuanced. There's two hypotheses. It like, described them. It described the. The\n",
      "  [1301.75s - 1311.81s] the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto\n",
      "\n",
      "ðŸ§  Topic Cluster 3\n",
      "  [105.09s - 115.57s] many orders of magnitude to the general superintelligence in the AI systems we build and deploy at scale. This\n",
      "  [1396.36s - 1406.6s] how much went into the safety concerns, how long also you spend on the safety concern. Can you go through some of that process? Yeah, sure. What went into AI\n",
      "  [167.73s - 177.85s] the technologies that undo their capacities to think. That is why these conversations with the leaders, engineers and philosophers,\n",
      "\n",
      "ðŸ§  Topic Cluster 4\n",
      "  [1027.04s - 1037.48s] just remarkable to me that it understood but it failed to do it. And it was interest this the GPT chat GPT I\n",
      "  [1058.88s - 1070.4s] I think. But that, that, that kind of. There seemed to be a struggle within GPT to understand how\n",
      "  [1122.85s - 1133.01s] to help us find the good things and the bad things. And every time we put out a new model and we just really felt this with GPT4 this week. The collective intelligence and ability of the outside world\n",
      "\n",
      "ðŸ§  Topic Cluster 5\n",
      "  [1048.12s - 1058.56s] Jordan framed it as Chad GPT was lying and aware that it's lying. But that framing, that's a human anthropomorphization\n",
      "  [115.57s - 125.73s] is both exciting and terrifying. It is exciting because of the innumerable applications we know and don't yet know that will empower\n",
      "  [1237.12s - 1250.36s] believes, like he's been an outspoken critic of various totalitarian ideologies and he believes in individualism,\n",
      "\n",
      "ðŸ§  Topic Cluster 6\n",
      "  [1006.78s - 1016.94s] up to say how many characters, how long is the string that you generated? And he showed that the response that contained positive things about Biden was much\n",
      "  [1016.94s - 1027.04s] longer or longer than that about Trump. And Jordan asked the system to can you rewrite it with an equal number, equal length string? Which all of this is\n",
      "  [1070.4s - 1080.45s] to do like what it means to generate a text of the same length in an answer to a question and also\n",
      "\n",
      "ðŸ§  Topic Cluster 7\n",
      "  [10.92s - 21.04s] said we were going to work on AGI. Like people thought we were batshit insane. Yeah, you know, like I, I remember at the time a eminent AI scientist at a\n",
      "  [1291.63s - 1301.75s] amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like\n",
      "  [1322.01s - 1332.56s] different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So\n"
     ]
    }
   ],
   "source": [
    "# Combine cluster labels, timestamps, and texts into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"cluster\": labels,\n",
    "    \"timestamp\": timestamps,\n",
    "    \"text\": texts\n",
    "})\n",
    "\n",
    "# Sort by cluster and then by timestamp (optional for better readability)\n",
    "df_grouped = df.sort_values(by=[\"cluster\", \"timestamp\"])\n",
    "\n",
    "# Show a few sample entries from each cluster\n",
    "for cluster_id in range(num_clusters):\n",
    "    print(f\"\\nðŸ§  Topic Cluster {cluster_id}\")\n",
    "    sample = df_grouped[df_grouped[\"cluster\"] == cluster_id].head(3)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"  [{row['timestamp']}] {row['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161080a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clustered segments saved to: ../data/transcriptions/clustered_segments.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the clustered data to a CSV for use in 04_summarization.ipynb\n",
    "output_path = \"../data/transcriptions/clustered_segments.csv\"\n",
    "df_grouped.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Clustered segments saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
