[1861.53s - 1872.69s] wanted, it wrote some code and that was it. Now you can have this back and forth dialogue where you can say, no, no, I meant this, or no, no, fix this bug or no, no, do this. And then of course the next version is the system can debug
[5817.89s - 5828.01s] in the world? I think the world is going to find out that if you can have 10 times as much code at the same price, you can just use even more to write even more code. The world just needs way more code. It is true that a lot
[4744.14s - 4754.19s] if created has a lot of power. How do you think we're doing? Like, honest. How do you think we're doing so far? Like, how do you think our decisions are? Like, do you think we're making things not better or worse? What can we do better? Well, the
[7670.59s - 7680.79s] about it. But it kind of reveals the fragility of our economic system. We may not be done. That may have been like the gun shown falling off the nightstand in the first scene of the movie or whatever. There could be like other banks for sure. There could
[8116.1s - 8126.17s] maybe it's in the data. Maybe we need to build better detectors which a really advanced AI could tell us how to do it may not be able to answer it on its Own, but it may be able to tell us what to go build
[6300.28s - 6310.48s] Now also how much of that can happen internally in one super intelligent AGI? Not so obvious. There is something about. Right, but there is something
[1301.75s - 1311.81s] the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto
[1322.01s - 1332.56s] different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So
[2187.46s - 2197.48s] Like, I don't think it would work to just say, like, hey, un go do this thing and we'll just take whatever you get back. Because we have like, a, we have responsibility of we're the one putting the system out, and if it breaks,
[5542.28s - 5552.4s] think I'm not a great, like, spokesperson for the AI movement. I'll say that I Think there could be like a more like there could be someone who enjoyed it more. There could be someone who's like much more charismatic. There could be someone who
[7648.11s - 7660.51s] people on Twitter were saying is like, well, it's their fault. They should have been like, you know, reading the balance sheet and the risk audit of the bank, like, do we really want people to have to do that? I would argue no. What
[1291.63s - 1301.75s] amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like
[4379.53s - 4389.54s] know, these people aren't very good and it's ridiculous to talk about AGI and I can't believe you're giving them time of day. And it's like that was the level of like pettiness and rancor in the field at a new group of people
[7250.05s - 7260.49s] a problem that is like very cool and that great people want to work on. We have great people and so people want to be around them. But even with that, I think there's just no shortcut for putting a ton of effort into
[1355.68s - 1365.83s] this is the thing that we get caught up in versus like what is this going to mean for our future? Now maybe you say this is critical to what this is going to mean for our future. The thing that it says
[1851.45s - 1861.53s] can ask it to adjust it. It's like, it's a. It's a weird different kind of way of debugging, I guess, for sure. The first versions of these systems were sort of, you know, one shot. You sort of, you said what you
[2639.43s - 2649.55s] billion. I heard GPT4 at 100 trillion. 100 trillion. Can I speak to this? Do you know that meme? Yeah, the big purple circle. Do you know where it originated? I don't do. I'd be curious to hear. It's the presentation I gave.
[2985.41s - 2995.57s] mean, I don't think we understand what that looks like. You said it's been six days. The thing that I am so excited about with this is not that it's a system that kind of goes off and does its own thing, but
[4864.46s - 4874.5s] person bothered. Like, I appreciate that. I feel all right about it. Of all the things I lose sleep over, it's not high on the list because it's important. There's a handful of companies, a handful of folks that are really pushing this
[5361.98s - 5372.1s] worldview look like for all kinds of groups of people that would answer this differently. I mean I have to do that constantly instead of like you've asked this a few times. But it's something I often do. You know, I ask people
