{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74d0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c81deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript_chunks = []\n",
    "\n",
    "        for entry in transcript:\n",
    "            start = entry['start']\n",
    "            end = entry['start'] + entry['duration']\n",
    "            text = entry['text']\n",
    "            transcript_chunks.append({\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text\n",
    "            })\n",
    "\n",
    "        return transcript_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e074fa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s - 11.72s] Heat. Heat.\n",
      "[2.84s - 11.72s] [Music]\n",
      "[16.06s - 34.49s] [Music]\n",
      "[34.80s - 39.84s] If you were to describe Ted the human in\n",
      "[38.00s - 42.84s] three words, what would the three words\n"
     ]
    }
   ],
   "source": [
    "# Example usage (replace with your own YouTube video ID)\n",
    "video_url = \"https://www.youtube.com/watch?v=QT2FGbR0nIM\"  # Example\n",
    "video_id = video_url.split(\"v=\")[-1]\n",
    "\n",
    "# Call the function and store the transcript\n",
    "transcript_data = get_youtube_transcript(video_id)\n",
    "\n",
    "# Print first 5 chunks for verification\n",
    "for entry in transcript_data[:5]:\n",
    "    print(f\"[{entry['start']:.2f}s - {entry['end']:.2f}s] {entry['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ed4ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(transcript_data, chunk_duration=120):\n",
    "    chunks = []\n",
    "    current_chunk = {\n",
    "        \"start\": transcript_data[0][\"start\"],\n",
    "        \"end\": transcript_data[0][\"end\"],\n",
    "        \"text\": transcript_data[0][\"text\"]\n",
    "    }\n",
    "\n",
    "    for entry in transcript_data[1:]:\n",
    "        if entry[\"start\"] - current_chunk[\"start\"] <= chunk_duration:\n",
    "            current_chunk[\"end\"] = entry[\"end\"]\n",
    "            current_chunk[\"text\"] += \" \" + entry[\"text\"]\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = {\n",
    "                \"start\": entry[\"start\"],\n",
    "                \"end\": entry[\"end\"],\n",
    "                \"text\": entry[\"text\"]\n",
    "            }\n",
    "\n",
    "    chunks.append(current_chunk)  # Append the last chunk\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "478ad386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1 [0.00s - 65.76s]:\n",
      "Heat. Heat. [Music] [Music] If you were to describe Ted the human in three words, what would the three words be? [Music] Hi Ted. Thank you. Good. Thanks for having me. Thank you for coming. Uh so...\n",
      "\n",
      "Chunk 2 [62.00s - 124.40s]:\n",
      "you've been in India for 12 hours now. About about 12 hours. So how does it feel? I love being coming to India so much and it's I never get enough time here. Usually I come and I've got to they've got me working and grinding me out and then shoot me back out back in the sky. But it's a it's a it's I...\n",
      "\n",
      "Chunk 3 [122.08s - 186.48s]:\n",
      "Angeles when his son was in school. Uh so we got to go out to dinner. So going having dinner with Sharakhan in India is much different than having dinner with him in in Los Angeles. Uh but so I would say I mean that's probably who someone I know the most and really enjoy working with. So you've been...\n"
     ]
    }
   ],
   "source": [
    "chunked_transcript = chunk_transcript(transcript_data, chunk_duration=60)\n",
    "\n",
    "# Print first 3 chunks for inspection\n",
    "for i, chunk in enumerate(chunked_transcript[:3]):\n",
    "    print(f\"\\nChunk {i+1} [{chunk['start']:.2f}s - {chunk['end']:.2f}s]:\\n{chunk['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcd1b52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 112\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of chunks:\",len(chunked_transcript))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15327587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Load the default model (you can switch later if needed)\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "501774d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indian: 0.3540\n",
      "heat: 0.3513\n",
      "bollywood: 0.3446\n",
      "energy: 0.3299\n",
      "bombay: 0.3263\n",
      "sharma: 0.3206\n",
      "india: 0.2970\n",
      "khan: 0.2864\n",
      "entertain: 0.2857\n",
      "lit: 0.2847\n",
      "bangjun: 0.2741\n",
      "arrive: 0.2702\n",
      "shah: 0.2688\n",
      "enjoying: 0.2659\n",
      "ruk: 0.2636\n",
      "passionate: 0.2574\n",
      "bring: 0.2536\n",
      "exciting: 0.2532\n",
      "kyunki: 0.2518\n",
      "traveling: 0.2492\n",
      "loving: 0.2480\n",
      "entertained: 0.2478\n",
      "fleeting: 0.2478\n",
      "entertaining: 0.2459\n",
      "passion: 0.2456\n",
      "ted: 0.2445\n",
      "travel: 0.2431\n",
      "power: 0.2407\n",
      "dinner: 0.2396\n",
      "satisfaction: 0.2391\n",
      "sensation: 0.2386\n",
      "love: 0.2355\n",
      "lover: 0.2321\n",
      "vacation: 0.2311\n",
      "feel: 0.2307\n",
      "reflective: 0.2304\n",
      "consume: 0.2283\n",
      "music: 0.2272\n",
      "meet: 0.2261\n",
      "bangalore: 0.2261\n",
      "bringing: 0.2251\n",
      "appetizing: 0.2243\n",
      "uta: 0.2243\n",
      "explore: 0.2241\n",
      "festival: 0.2240\n",
      "adventurous: 0.2237\n",
      "touring: 0.2236\n",
      "phenomenal: 0.2229\n",
      "burned: 0.2227\n",
      "met: 0.2222\n"
     ]
    }
   ],
   "source": [
    "# Join all chunk texts into one large document\n",
    "full_text = \" \".join([chunk[\"text\"] for chunk in chunked_transcript])\n",
    "\n",
    "# Extract keywords (phrases), adjust top_n as needed\n",
    "keywords = kw_model.extract_keywords(full_text, top_n=50, stop_words='english')\n",
    "\n",
    "# Print top 10 keywords for inspection\n",
    "for kw, score in keywords[:50]:\n",
    "    print(f\"{kw}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa2df59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flat list of keywords (not scores), for fast matching\n",
    "top_keywords = [kw[0].lower() for kw in keywords]  # Only keep the keyword string, lowercase for match\n",
    "\n",
    "# Score each chunk by how many top keywords appear in it\n",
    "def score_chunks_by_keywords(chunks, keywords):\n",
    "    scored_chunks = []\n",
    "    for chunk in chunks:\n",
    "        text = chunk[\"text\"].lower()\n",
    "        keyword_hits = sum(1 for kw in keywords if kw in text)\n",
    "        scored_chunks.append({\n",
    "            \"start\": chunk[\"start\"],\n",
    "            \"end\": chunk[\"end\"],\n",
    "            \"text\": chunk[\"text\"],\n",
    "            \"keyword_score\": keyword_hits\n",
    "        })\n",
    "    return scored_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2734fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Score: 11 | Time: 62.00s - 124.40s\n",
      "\n",
      "you've been in India for 12 hours now. About about 12 hours. So how does it feel? I love being coming to India so much and it's I never get enough time here. Usually I come and I've got to they've got me working and grinding me out and then shoot me back out back in the sky. But it's a it's a it's I love the energy of India. What have you done so far today? Today I uh came in I met with the ministers and I met I went to the waves conference and spoke on stage and was interviewed. Uh do you have a favorite creator in India? That's like you know we work with so many it'd be like picking your kids but you know picking your favorite child almost. But I have to tell you early early in coming to India I met uh Shah Ruk Khan right away early and we just he hosted a very nice little dinner for for me and we just hit it off immediately and uh I've come back since and my with my wife and we've had a a nice times together. Uh he we visited with each other in Los...\n",
      "\n",
      "Rank 2 | Score: 7 | Time: 2580.40s - 2643.68s\n",
      "\n",
      "does not have access to a movie theater. The experience where for $20 a movie, a family of three or four can be entertained in a safe environment with air conditioning and food if you choose to Yeah. access food and all of that for 3 hours or four [Music] hours. Uh it sounds very appetizing to me from an experience standpoint for a vast majority of India. But in theory it is not playing out like that maybe because I don't think movie theaters are doing as well as they were doing five or 10 years ago. And you know it's interesting almost everything else for like live events is back bigger than pre-COVID live concerts sporting events all these things all bigger. So I I actually think consumers go to the went to the went back to the theater and said this isn't much better than watching at home where you go to a concert it's a totally different experience than...\n",
      "\n",
      "Rank 3 | Score: 7 | Time: 3815.84s - 3879.20s\n",
      "\n",
      "Hulu became the DV the TV company. Mhm. So that's why we kind of got into TV at the same time, licensing TV shows. What happens to old school TV? The TV that I grew up on, like the TV where I used to go I still remember I used to go home from school at 300 p.m. and watch this particular cartoon called SWAT Cats and Powerpuff Girls and Teenage Mut and I used to Teenage Mutant Ninja Turtles. I remember watching all these Indian soaps, you know, with my mother like late in the night, Kyunki, Sasby and all of that. What happens to that entire industry? I mean, I guess a lot of it shows up in in retro channels. I think that there'll be an economic model where we we'll have more of it over time. Um, these fast services that are mostly ad supported. I think those are mostly delivering most of those episodes to people. YouTube certainly does some of that, right? So, But I feel like there's a lot of different ways to get to it. No one's going to be making...\n",
      "\n",
      "Rank 4 | Score: 7 | Time: 4676.40s - 4738.96s\n",
      "\n",
      "to go to that table.\" So I just leaned into him. I go, \"Who is this?\" He goes, \"What do you mean?\" He goes, I go, \"Who is this woman? She's amazing.\" And he goes, \"Uh, how do you not know her? This she's this Nicole Avon. How do you not know Nicole Avon?\" And I go, \"I don't know her.\" He goes, \"Oh, I'll introduce you to her later.\" So, at the end of the night, he we left the event and went to a dinner, right? 10 of 10 people. And I'm sitting there talking to my friend Lawrence Bender, who hosted the table. And he said, \"Um, oh, here comes Nicole. Um, and she came in by herself.\" And she goes, he goes, \"Switch seats so you can talk to her. She's great. And we switched seats and I sit and talked to her. We talked for three and a half hours and eight months later we're married. Wow. She's an amazing woman. Met Nicole. I mean, yeah, I'd love to meet her. Yeah, she's really phenomenal. So Netflix in India, you don't have significant market share yet. How important is it for you as a market and how are you approaching it? like what is working for you in...\n",
      "\n",
      "Rank 5 | Score: 7 | Time: 6210.88s - 6274.40s\n",
      "\n",
      "other things. So, um, we're broadening the offering, not narrowing it to something else. What is what is working like all the data you have on India now, which is significant in all the time that you've been here. Can you tell me some nonobvious insights you have learned that I won't know? I I'm sure that you would probably be the the international performance of con of programming from Japanese anime uh to Turkish soaps to all kinds of different things that you would not expect would work very well in India. Work very well in India. Is that we don't speak the language? I I I I think for some reason and you probably understand better than I do, the audience has got very is very hungry for diverse storytelling from around the world. They love Bollywood. They love South Indian action movies. They love all those all local content for sure, but far more adventurous as viewers than I expected. Could it go...\n"
     ]
    }
   ],
   "source": [
    "# Run the scoring function\n",
    "scored_chunks = score_chunks_by_keywords(chunked_transcript, top_keywords)\n",
    "\n",
    "# Sort by keyword density\n",
    "ranked_chunks = sorted(scored_chunks, key=lambda x: x[\"keyword_score\"], reverse=True)\n",
    "\n",
    "# Show top 5 most keyword-dense chunks\n",
    "for i, chunk in enumerate(ranked_chunks[:5]):\n",
    "    print(f\"\\nRank {i+1} | Score: {chunk['keyword_score']} | Time: {chunk['start']:.2f}s - {chunk['end']:.2f}s\\n\")\n",
    "    print(chunk['text'] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3f8fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the sentence embedding model\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce4d95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join top keywords into a single \"summary string\"\n",
    "global_theme_text = \" \".join([kw for kw in top_keywords[:30]])\n",
    "global_theme_emb = embed_model.encode(global_theme_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d3f214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embedding_scores(chunks, global_emb, model):\n",
    "    for chunk in chunks:\n",
    "        chunk_emb = model.encode(chunk[\"text\"], convert_to_tensor=True)\n",
    "        sim_score = util.pytorch_cos_sim(chunk_emb, global_emb).item()\n",
    "        chunk[\"embedding_score\"] = sim_score\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "441c1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_chunks = add_embedding_scores(scored_chunks, global_theme_emb, embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "620b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_score_chunks(chunks, weight_keywords=0.5, weight_embedding=0.5):\n",
    "    # Normalize scores for fair combination\n",
    "    max_kw = max(chunk[\"keyword_score\"] for chunk in chunks)\n",
    "    max_emb = max(chunk[\"embedding_score\"] for chunk in chunks)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        norm_kw = chunk[\"keyword_score\"] / max_kw if max_kw > 0 else 0\n",
    "        norm_emb = chunk[\"embedding_score\"] / max_emb if max_emb > 0 else 0\n",
    "        chunk[\"hybrid_score\"] = weight_keywords * norm_kw + weight_embedding * norm_emb\n",
    "\n",
    "    return sorted(chunks, key=lambda x: x[\"hybrid_score\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75b540ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | Hybrid Score: 1.0000 | Time: 62.00s - 124.40s\n",
      "\n",
      "you've been in India for 12 hours now. About about 12 hours. So how does it feel? I love being coming to India so much and it's I never get enough time here. Usually I come and I've got to they've got me working and grinding me out and then shoot me back out back in the sky. But it's a it's a it's I...\n",
      "\n",
      "Rank 2 | Hybrid Score: 0.7293 | Time: 6824.48s - 6880.44s\n",
      "\n",
      "away. Yeah. And I feel like it's never been more true right now. I mean, it just feels like there India is on a precipice of something very big. Uh and this is probably the the most exciting time. uh that I've been aware of in India's history. Done. All right. Thank you. Good time. Thanks everyone. ...\n",
      "\n",
      "Rank 3 | Hybrid Score: 0.6763 | Time: 2580.40s - 2643.68s\n",
      "\n",
      "does not have access to a movie theater. The experience where for $20 a movie, a family of three or four can be entertained in a safe environment with air conditioning and food if you choose to Yeah. access food and all of that for 3 hours or four [Music] hours. Uh it sounds very appetizing to me fr...\n",
      "\n",
      "Rank 4 | Hybrid Score: 0.6572 | Time: 6210.88s - 6274.40s\n",
      "\n",
      "other things. So, um, we're broadening the offering, not narrowing it to something else. What is what is working like all the data you have on India now, which is significant in all the time that you've been here. Can you tell me some nonobvious insights you have learned that I won't know? I I'm sur...\n",
      "\n",
      "Rank 5 | Hybrid Score: 0.6412 | Time: 5596.40s - 5656.32s\n",
      "\n",
      "he said when we say Jess it was a long delay there was a lot of delays it was a complicated production he did things uh built these enormous practical sets that we shot on. It was something beautifully old-fashioned about every element of the production. Uh, and I was just he was so proud of and I w...\n"
     ]
    }
   ],
   "source": [
    "ranked_chunks = hybrid_score_chunks(scored_chunks, weight_keywords=0.5, weight_embedding=0.5)\n",
    "\n",
    "# Preview top results\n",
    "for i, chunk in enumerate(ranked_chunks[:5]):\n",
    "    print(f\"\\nRank {i+1} | Hybrid Score: {chunk['hybrid_score']:.4f} | Time: {chunk['start']:.2f}s - {chunk['end']:.2f}s\\n\")\n",
    "    print(chunk['text'][:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a2f6ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49f12b3963a4d258242fa2b6772f5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhes\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuhes\\.cache\\huggingface\\hub\\datasets--vwxyzjn--summarize_from_feedback_tldr_3_filtered. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c914f43927247ffb5345bc716057451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/186M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b92a5c76610487c820bc35e7392a2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid.jsonl:   0%|          | 0.00/10.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992a412df71e498ebd56ab853b9774d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/10.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adf853fa3734c0692c9799beb28bfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/116722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af5aeec8a174cedb0cf34bbda90d95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/6447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072401ca736d41bf91c8d53d3b3c6ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"vwxyzjn/summarize_from_feedback_tldr_3_filtered\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "280daa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 't3_1hxu8s',\n",
       " 'subreddit': 'relationships',\n",
       " 'title': 'I (f/22) have to figure out if I want to still know these girls or not and would hate to sound insulting',\n",
       " 'post': \"Not sure if this belongs here but it's worth a try. \\n\\nBackstory:\\nWhen I (f/22) went through my first real breakup 2 years ago because he needed space after a year of dating roand  it effected me more than I thought. It was a horrible time in my life due to living with my mother and finally having the chance to cut her out of my life. I can admit because of it was an emotional wreck and this guy was stable and didn't know how to deal with me. We ended by him avoiding for a month or so after going to a festival with my friends. When I think back I wish he just ended. So after he ended it added my depression I suffered but my friends helped me through it and I got rid of everything from him along with cutting contact. \\n\\nNow: Its been almost 3 years now and I've gotten better after counselling and mild anti depressants. My mother has been out of my life since then so there's been alot of progress. Being stronger after learning some lessons there been more insight about that time of my life but when I see him or a picture everything comes back. The emotions and memories bring me back down. \\n\\nHis friends (both girls) are on my facebook because we get along well which is hard to find and I know they'll always have his back. But seeing him in a picture or talking to him at a convention having a conversation is tough. Crying confront of my current boyfriend is something I want to avoid. \\n\\nSo I've been thinking that I have to cut contact with these girls because it's time to move on because it's healthier. It's best to avoid him as well. But will they be insulted? Will they accept it? Is there going to be awkwardness? I'm not sure if it's the right to do and could use some outside opinions.\",\n",
       " 'summary': \"I still have contact with an old ex's friends but can't stand to see or talk to him. His friends are really nice ,so how do I tell them I possibly want to unfriend them on Facebook because of him?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]  # Shows the first example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4bb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea3605640bb47678f591255f92412cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/116722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_for_t5(example):\n",
    "    return {\n",
    "        \"input\": f\"Summarize this: {example['post']}\",\n",
    "        \"output\": example['summary']\n",
    "    }\n",
    "\n",
    "# Apply formatting to each split\n",
    "formatted_dataset = dataset.map(format_for_t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d55e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'subreddit', 'title', 'post', 'summary', 'input', 'output'],\n",
      "    num_rows: 116722\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8882fb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14d6e4ac32d47aa94b2c0422acade1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhes\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yuhes\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e10c2a89eef42fb92661fb9456fa598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3379c723b614a69b2b6d77144cdddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7697f605db744e09ec005a8b0060a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe0c7100db34e139a80dc037a93e04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c81c04157545f9b99e109b98687631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e79afad8af4453afbf5b380ccb2876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Step 1: Load the base model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Step 2: Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],  # or [\"q_proj\", \"v_proj\"] depending on the model\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Since we're doing summarization\n",
    ")\n",
    "\n",
    "# Step 3: Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b401d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    # Tokenize input without truncating\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"],\n",
    "        padding=\"longest\",   # or \"max_length\" if batching later\n",
    "        truncation=False     # disables cutting off long inputs\n",
    "    )\n",
    "\n",
    "    # Tokenize output with reasonable max_length (still keep short summaries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"output\"],\n",
    "            padding=\"longest\",   # to pad to longest example in batch\n",
    "            truncation=True,\n",
    "            max_length=128       # summaries should remain concise\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "122f8051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3432d791bf2f406ca4a875a0471a4ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/116722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhes\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  self._in_target_context_manager = False\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b321da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset[:1000] # Limit to 1000 examples for faster training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6fbadeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'subreddit', 'title', 'post', 'summary', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['id', 'subreddit', 'title', 'post', 'summary', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert dict to HuggingFace Dataset\n",
    "tokenized_hf_dataset = Dataset.from_dict(tokenized_dataset)\n",
    "\n",
    "# 90% train, 10% validation\n",
    "dataset_split = tokenized_hf_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Wrap into a DatasetDict for easier access later\n",
    "dataset_split = DatasetDict({\n",
    "    \"train\": dataset_split[\"train\"],\n",
    "    \"eval\": dataset_split[\"test\"]\n",
    "})\n",
    "\n",
    "# Quick check\n",
    "print(dataset_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1e23ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# Create train and eval dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_split[\"train\"],\n",
    "    batch_size=4,  # You can adjust depending on your GPU\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset_split[\"eval\"],\n",
    "    batch_size=4,\n",
    "    collate_fn=default_data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e6ce8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Automatically choose GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# Use AdamW optimizer for fine-tuning transformers\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d848b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the keys that matter\n",
    "columns_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "# Remove all other columns\n",
    "dataset_split = dataset_split.remove_columns(\n",
    "    [col for col in dataset_split[\"train\"].column_names if col not in columns_to_keep]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2b9ca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_split[\"train\"],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset_split[\"eval\"],\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3222595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/225 [00:09<33:52,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Loss: 3.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/225 [16:29<35:28, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 - Loss: 3.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 201/225 [36:42<03:28,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 - Loss: 2.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [39:58<00:00, 10.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d8e99b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:57<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.2644\n",
      "rouge2: 0.0772\n",
      "rougeL: 0.2018\n",
      "rougeLsum: 0.2020\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    # Move to device\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # Generate summaries\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    # Store for metric\n",
    "    predictions.extend(decoded_preds)\n",
    "    references.extend(decoded_labels)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "25175895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/flan_t5_lora_summary\\\\tokenizer_config.json',\n",
       " '../models/flan_t5_lora_summary\\\\special_tokens_map.json',\n",
       " '../models/flan_t5_lora_summary\\\\tokenizer.json')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the LoRA adapter weights\n",
    "model.save_pretrained(\"../models/flan_t5_lora_summary\")\n",
    "\n",
    "# Save the tokenizer for future use\n",
    "tokenizer.save_pretrained(\"../models/flan_t5_lora_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6c7179b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§© Chunk 1 | Time: 62.00s - 124.40s\n",
      "ðŸ“œ Summary: I love being in India so much and it's I never get enough time here. What have you done so far?\n",
      "\n",
      "ðŸ§© Chunk 2 | Time: 6824.48s - 6880.44s\n",
      "ðŸ“œ Summary: I've watched the first two watched the first two episodes so far. What do you think of it?\n",
      "\n",
      "ðŸ§© Chunk 3 | Time: 2580.40s - 2643.68s\n",
      "ðŸ“œ Summary: I don't think movie theaters are doing as well as they were doing five or 10 years ago.\n",
      "\n",
      "ðŸ§© Chunk 4 | Time: 6210.88s - 6274.40s\n",
      "ðŸ“œ Summary: I'm sure that you have learned some non-obvious insights you have learned that I won't know.\n",
      "\n",
      "ðŸ§© Chunk 5 | Time: 5596.40s - 5656.32s\n",
      "ðŸ“œ Summary: I'm going to try it. Can you take spice?\n",
      "\n",
      "ðŸ§© Chunk 6 | Time: 2028.56s - 2092.80s\n",
      "ðŸ“œ Summary: You're saying Bollywood should not try and sell or make the stories of Hollywood and Hollywood should not try and make the stories of Bollywood.\n",
      "\n",
      "ðŸ§© Chunk 7 | Time: 4737.28s - 4801.20s\n",
      "ðŸ“œ Summary: India and what is not working for you in India?\n",
      "\n",
      "ðŸ§© Chunk 8 | Time: 4859.16s - 4922.16s\n",
      "ðŸ“œ Summary: I don't know much about Bollywood like I know a little.\n",
      "\n",
      "ðŸ§© Chunk 9 | Time: 0.00s - 65.76s\n",
      "ðŸ“œ Summary: Hi, Ted. Thank you.\n",
      "\n",
      "ðŸ§© Chunk 10 | Time: 2150.72s - 2213.20s\n",
      "ðŸ“œ Summary: You're not selling the organization you belong to beyond a point.\n",
      "\n",
      "ðŸ§© Chunk 11 | Time: 5534.96s - 5598.16s\n",
      "ðŸ“œ Summary: Sanjini Leven came into the to LA to do the pitch I almost think he wasn't going to make a TV show\n",
      "\n",
      "ðŸ§© Chunk 12 | Time: 6150.00s - 6213.44s\n",
      "ðŸ“œ Summary: I don't believe that there's any conflict with getting the best person and having a diverse workforce.\n",
      "\n",
      "ðŸ§© Chunk 13 | Time: 3815.84s - 3879.20s\n",
      "ðŸ“œ Summary: I think that there's a lot of different ways to get to it.\n",
      "\n",
      "ðŸ§© Chunk 14 | Time: 4676.40s - 4738.96s\n",
      "ðŸ“œ Summary: Netflix in India, you don't have significant market share yet and how are you approaching it?\n",
      "\n",
      "ðŸ§© Chunk 15 | Time: 2456.88s - 2520.24s\n",
      "ðŸ“œ Summary: I'm going to digress a bit and ask you a question.\n",
      "\n",
      "ðŸ§© Chunk 16 | Time: 4182.32s - 4247.84s\n",
      "ðŸ“œ Summary: Is that the way I aspiring to create content?\n",
      "\n",
      "ðŸ§© Chunk 17 | Time: 122.08s - 186.48s\n",
      "ðŸ“œ Summary: Netflix for 25 years and I've been watching a lot of his interviews.\n",
      "\n",
      "ðŸ§© Chunk 18 | Time: 5473.20s - 5537.52s\n",
      "ðŸ“œ Summary: Are you hard on yourself in the work realm or in the family realm?\n",
      "\n",
      "ðŸ§© Chunk 19 | Time: 4244.44s - 4310.48s\n",
      "ðŸ“œ Summary: I think there might be a 100 or 200 million people in India who can actually afford to spend that premium towards an additional expense in the household, right?\n",
      "\n",
      "ðŸ§© Chunk 20 | Time: 6762.88s - 6827.20s\n",
      "ðŸ“œ Summary: You're going to have to learn how all those things operate and how all those people work.\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to inference mode\n",
    "\n",
    "for i, chunk in enumerate(ranked_chunks[:20]):\n",
    "    input_text = f\"Summarize this: {chunk['text']}\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,  # adjust if needed\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Decode output\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\nðŸ§© Chunk {i+1} | Time: {chunk['start']:.2f}s - {chunk['end']:.2f}s\")\n",
    "    print(\"ðŸ“œ Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef91c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
