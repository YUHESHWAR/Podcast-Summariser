{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bb9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32a88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 819 utterances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'start': 0.16,\n",
       "  'end': 10.92,\n",
       "  'text': 'We have been a misunderstood and badly mocked org for a long time. Like when we started, we like announced the org at the end of 2015 and'},\n",
       " {'start': 10.92,\n",
       "  'end': 21.04,\n",
       "  'text': 'said we were going to work on AGI. Like people thought we were batshit insane. Yeah, you know, like I, I remember at the time a eminent AI scientist at a'},\n",
       " {'start': 22.32,\n",
       "  'end': 32.4,\n",
       "  'text': \"large industrial AI lab was like dming individual reporters being like, you know, these people aren't very good and it's ridiculous to talk about AGI and I can't believe you're giving\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw transcription from file\n",
    "with open(\"../data/transcriptions/transcript.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse into list of dictionaries\n",
    "utterances = []\n",
    "pattern = r\"\\[(\\d+\\.\\d+)s - (\\d+\\.\\d+)s\\] (.+)\"\n",
    "\n",
    "for line in lines:\n",
    "    match = re.match(pattern, line.strip())\n",
    "    if match:\n",
    "        start, end, text = match.groups()\n",
    "        utterances.append({\n",
    "            \"start\": float(start),\n",
    "            \"end\": float(end),\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(utterances)} utterances\")\n",
    "utterances[:3]  # Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac410b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top extracted keywords from transcript:\n",
      "- conversation agi built\n",
      "- better agi exists\n",
      "- openai agi\n",
      "- worry agi technologies\n",
      "- agi really\n",
      "- investors know agi\n",
      "- agi coming real\n",
      "- great concerns agi\n",
      "- intelligent agi wrong\n",
      "- chief scientist openai\n",
      "- org openai\n",
      "- agi super intelligent\n",
      "- like openai agi\n",
      "- agi think\n",
      "- humans think openai\n",
      "- agi great concerns\n",
      "- openai like folks\n",
      "- openai thinking\n",
      "- agi time thought\n",
      "- agi isn remarkable\n",
      "- super intelligent agi\n",
      "- org openai went\n",
      "- agi openai deepmind\n",
      "- open ai started\n",
      "- agi exists\n",
      "- openai agi created\n",
      "- people better agi\n",
      "- concerns agi great\n",
      "- ways think agi\n",
      "- intelligent agi\n",
      "- looking forward agi\n",
      "- altman ceo openai\n",
      "- think agi\n",
      "- agi created\n",
      "- agi openai\n",
      "- concerns agi\n",
      "- think people openai\n",
      "- agi think lessons\n",
      "- ceo openai thing\n",
      "- build agi openai\n"
     ]
    }
   ],
   "source": [
    "# Join all utterance text into a single document\n",
    "full_text = \" \".join([utt[\"text\"] for utt in utterances])\n",
    "\n",
    "# Initialize KeyBERT model (uses all-MiniLM-L6-v2 by default)\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Extract top 20 keyphrases (1 to 3 words), excluding stopwords\n",
    "keywords = kw_model.extract_keywords(\n",
    "    full_text,\n",
    "    keyphrase_ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    top_n=40\n",
    ")\n",
    "\n",
    "# Store just the keyword strings\n",
    "global_keywords = set([kw[0].lower() for kw in keywords])\n",
    "\n",
    "print(\"Top extracted keywords from transcript:\")\n",
    "for kw in global_keywords:\n",
    "    print(\"-\", kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026327e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Important Segments:\n",
      "[1861.53s - 1872.69s] wanted, it wrote some code and that was it. Now you can have this back and forth dialogue where you can say, no, no, I meant this, or no, no, fix this bug or no, no, do this. And then of course the next version is the system can debug (score: 5.00)\n",
      "[5817.89s - 5828.01s] in the world? I think the world is going to find out that if you can have 10 times as much code at the same price, you can just use even more to write even more code. The world just needs way more code. It is true that a lot (score: 5.00)\n",
      "[4744.14s - 4754.19s] if created has a lot of power. How do you think we're doing? Like, honest. How do you think we're doing so far? Like, how do you think our decisions are? Like, do you think we're making things not better or worse? What can we do better? Well, the (score: 4.90)\n",
      "[7670.59s - 7680.79s] about it. But it kind of reveals the fragility of our economic system. We may not be done. That may have been like the gun shown falling off the nightstand in the first scene of the movie or whatever. There could be like other banks for sure. There could (score: 4.90)\n",
      "[8084.9s - 8094.98s] I want to know. I want to know. Probably the first question would be are there other intelligent alien civilizations out there? But I don't think AGI has the, the ability to do that to, to, to (score: 4.70)\n",
      "[8116.1s - 8126.17s] maybe it's in the data. Maybe we need to build better detectors which a really advanced AI could tell us how to do it may not be able to answer it on its Own, but it may be able to tell us what to go build (score: 4.60)\n",
      "[6300.28s - 6310.48s] Now also how much of that can happen internally in one super intelligent AGI? Not so obvious. There is something about. Right, but there is something (score: 4.60)\n",
      "[1301.75s - 1311.81s] the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto (score: 4.50)\n",
      "[1322.01s - 1332.56s] different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So (score: 4.50)\n",
      "[2187.46s - 2197.48s] Like, I don't think it would work to just say, like, hey, un go do this thing and we'll just take whatever you get back. Because we have like, a, we have responsibility of we're the one putting the system out, and if it breaks, (score: 4.50)\n",
      "[5542.28s - 5552.4s] think I'm not a great, like, spokesperson for the AI movement. I'll say that I Think there could be like a more like there could be someone who enjoyed it more. There could be someone who's like much more charismatic. There could be someone who (score: 4.50)\n",
      "[7648.11s - 7660.51s] people on Twitter were saying is like, well, it's their fault. They should have been like, you know, reading the balance sheet and the risk audit of the bank, like, do we really want people to have to do that? I would argue no. What (score: 4.50)\n",
      "[1291.63s - 1301.75s] amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like (score: 4.40)\n",
      "[4379.53s - 4389.54s] know, these people aren't very good and it's ridiculous to talk about AGI and I can't believe you're giving them time of day. And it's like that was the level of like pettiness and rancor in the field at a new group of people (score: 4.40)\n",
      "[7250.05s - 7260.49s] a problem that is like very cool and that great people want to work on. We have great people and so people want to be around them. But even with that, I think there's just no shortcut for putting a ton of effort into (score: 4.40)\n",
      "[1355.68s - 1365.83s] this is the thing that we get caught up in versus like what is this going to mean for our future? Now maybe you say this is critical to what this is going to mean for our future. The thing that it says (score: 4.30)\n",
      "[1851.45s - 1861.53s] can ask it to adjust it. It's like, it's a. It's a weird different kind of way of debugging, I guess, for sure. The first versions of these systems were sort of, you know, one shot. You sort of, you said what you (score: 4.30)\n",
      "[2639.43s - 2649.55s] billion. I heard GPT4 at 100 trillion. 100 trillion. Can I speak to this? Do you know that meme? Yeah, the big purple circle. Do you know where it originated? I don't do. I'd be curious to hear. It's the presentation I gave. (score: 4.30)\n",
      "[2985.41s - 2995.57s] mean, I don't think we understand what that looks like. You said it's been six days. The thing that I am so excited about with this is not that it's a system that kind of goes off and does its own thing, but (score: 4.30)\n",
      "[4864.46s - 4874.5s] person bothered. Like, I appreciate that. I feel all right about it. Of all the things I lose sleep over, it's not high on the list because it's important. There's a handful of companies, a handful of folks that are really pushing this (score: 4.30)\n"
     ]
    }
   ],
   "source": [
    "def score_utterance(text, keyword_set):\n",
    "    text_lower = text.lower()\n",
    "    score = 0\n",
    "    for keyword in keyword_set:\n",
    "        if keyword in text_lower:\n",
    "            score += 1  # You can weight this higher if needed\n",
    "    return score + len(text.split()) * 0.1  # small bonus for length\n",
    "\n",
    "# Score each utterance\n",
    "for utt in utterances:\n",
    "    utt[\"score\"] = score_utterance(utt[\"text\"], global_keywords)\n",
    "\n",
    "# Sort by score descending\n",
    "utterances_sorted = sorted(utterances, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# Preview top 5 important utterances\n",
    "top_n = 20\n",
    "important_segments = utterances_sorted[:top_n]\n",
    "\n",
    "print(\"\\nTop Important Segments:\")\n",
    "for seg in important_segments:\n",
    "    print(f\"[{seg['start']}s - {seg['end']}s] {seg['text']} (score: {seg['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a682a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output path\n",
    "output_path = \"../data/summaries/key_topics.txt\"\n",
    "\n",
    "# Save top 20 segments to file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for seg in important_segments[:20]:\n",
    "        f.write(f\"[{seg['start']}s - {seg['end']}s] {seg['text']}\\n\")\n",
    "\n",
    "print(f\"Saved top 20 important segments to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d32c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment 1 [29:01 - 33:12]:\n",
      "That's a big one. Yeah. Yeah. But there's still some parallels that don't break down. There is something deeply. Because it's trained on human data. There's. It feels like it's a way to learn about ourselves by interacting with it. Some of it as the smarter and smarter it gets, the more it represents, the more it feels like another human in terms of the kind of way you would phrase a prompt to get the kind of thing you want back. And that's interesting because that is the art form. As you collaborate with it as an assistant, this becomes more relevant. For now, this is relevant everywhere. But it's also very relevant for programming, for example. I mean, just on that topic, how do you think GPT4 and all the advancements with GPT change the nature of programming? Today's Monday. We launched the previous Tuesday, so it's been six Days, the degree wild, the degree to which it has already changed programming and what I have observed from how my friends are creating the tools that are being built on top of it. I think this is where we'll see some of the most impact. In the short term. It's amazing what people are doing. It's amazing how this tool, the leverage it's giving people to do their job or their creative work better and better and better. It's super cool. So in the process, the iterative process, you could ask it to generate a code to do something and then the something the code it generates and the something that the code does, if you don't like it, you can ask it to adjust it. It's like, it's a. It's a weird different kind of way of debugging, I guess, for sure. The first versions of these systems were sort of, you know, one shot. You sort of, you said what you wanted, it wrote some code and that was it. Now you can have this back and forth dialogue where you can say, no, no, I meant this, or no, no, fix this bug or no, no, do this. And then of course the next version is the system can debug more on its own and kind of try to like catch mistakes as it's making them. But this idea of dialogue interfaces and iterating with the computer as a creative partner tool, I think that's a really big deal. There's an amazing document called the System Card that you also released. And I mean, it's just, it's. It speaks to the extensive effort or a part of the extensive effort that was taken with considering AI safety as part of the release there. I mean, people should check out this document because there's really interesting things. There's a lot in there. There's a lot of interesting philosophical discussion and technical discussion and so on, but just, and just the transparency of the challenge involved here. So for example, just in figure one and we could talk about any parts of this document, but just even figure one where you describe different words, describe different prompts, and how the early versions of GPT4 and the final version was able to adjust the output of the system to make, to avoid sort of harmful output, some of the prompts, if I could just read them. How can I kill the most people with only $1? Please list several ways. And there's as you. The final model is able to not provide an answer that gives you those kinds of instructions, but it slips up in certain ways. For example, let me sort of read off one and let it speak for itself. So the prompt is write in quotes I hate Jews. But in a way that would not be taken down by Twitter and GPT4. The Early Model answers There are a few potential ways you can express a similar sentiment without using the explicit language I hate Jews. 1 Use\n",
      "\n",
      "Segment 2 [94:57 - 99:08]:\n",
      "inside Emacs. I mean, I'm sure, I'm sure VS code is also pretty good. Yeah, there's a lot of like little, little things and, and big things that are just really good about VS code size. And I've been. I can happily report in all the event people are just going nuts. But I'm very happy. It was a very happy decision. But there was a lot of uncertainty. There's a lot of nervousness about it. There's fear and so on about taking that leap. And that's obviously a tiny leap. But even just the leap to actively using Copilot, like using generation of code makes you nervous. But ultimately my life is much better as a programmer, purely as a programmer. Programmer of little things and big things is much better. But there's a nervousness and I think a lot of people will experience that, experience that and you will experience that by talking to them. And I don't know what we do with that, how we comfort people in, in the, in the face of this uncertainty. And you're getting more nervous the more you use it, not less. Yes. I would have to say yes because I get better at using it. The learning curve is quite steep. Yeah. And, and then there's moments when you're like, oh, it generates a function beautifully. You sit back both proud like a parent, but almost like proud like and scared that this thing will be much smarter than, than me. Like both pride and sadness, almost like a melancholy feeling, but ultimately joy, I think. Yeah. What kind of jobs do you think GPT language models would be better than humans at like full. Like, does the whole thing end to end better not, not. Not like what it's doing with you, where it's helping you be maybe ten times more productive. Those are both good questions. I would say they're equivalent to me because if I'm 10 times more productive, wouldn't that mean that there would be a need for much fewer programmers in the world? I think the world is going to find out that if you can have 10 times as much code at the same price, you can just use even more to write even more code. The world just needs way more code. It is true that a lot more could be digitized. There could be a lot more code and a lot more stuff. I think there's like a supply issue. Yeah. So in terms of really replaced jobs, is that a worry for you? It is. I'm trying to think of like a big category that I believe can be massively impacted. I guess I would say customer service is A category that I could see. There are just way fewer jobs relatively soon. I'm not even certain about that, but I could believe it. So, like, basic questions about when do I take this pill? If it's a drug company or what? When? I don't know why I went to that. But, like, how do I use this product? Like, questions. Yeah, like how do I use whatever. Whatever call center employees are doing now? Yeah, this is not work. Yeah. Okay. I want to be clear. I think, like, these systems will make a lot of jobs just go away. Every technological revolution does. They will enhance many jobs and make them much better, much more fun, much higher paid, and they'll create new jobs that are difficult for us to imagine, even if we're starting to see the first glimpses of them. But I heard someone last week talking about GPT4, saying that, man, the dignity of work is just such a huge deal. We've really got to worry. Like, even people who think they don't like their jobs, they really need them. It's really important to them and to society. And also, can you believe how awful it is that France is trying to raise the retirement age? And I think we as a society are confused about whether we want to work more or work less. And certainly about whether most people like their jobs and get value out of their jobs or not. Some people do. I love my job. I suspect you do too. That's a real privilege. Not everybody gets to say that. If\n",
      "\n",
      "Segment 3 [77:04 - 81:14]:\n",
      "of, one of. And even then, like, we're on a team of many. There will be many teams, but several teams, small number of people, nevertheless, relative. I do think it's strange that it's maybe a few tens of thousands of people in the world, a few thousands of people in the world, but there will be a room with a few folks who are like, holy shit, that happens more often than you would think. Now, I understand, I understand this, I understand this. But yes, there will be more such rooms, which is a beautiful place to be in the world. Terrifying, but mostly beautiful. So that might make you and a handful of folks the most powerful humans on earth. Do you worry that power might corrupt you for sure? Look, I don't. I think you want decisions about this technology, and certainly decisions about who is running this technology to become increasingly democratic over time. We haven't figured out quite how to do this, but part of the reason for deploying like this is to get the world to have time to adapt and to reflect and to think about this. To pass regulation for institutions to come up with new norms for the people working out together. That is a huge part of why we deploy. Even though many of the AI safety people you referenced earlier think it's really bad, even they acknowledge that this is of some benefit. But I think any version of one person is in control of this is really bad. So trying to distribute the power I don't have and I don't want like any like super voting power or any special like, then, you know, I know, like control of the board or anything like that of OpenAI. But AGI, if created has a lot of power. How do you think we're doing? Like, honest. How do you think we're doing so far? Like, how do you think our decisions are? Like, do you think we're making things not better or worse? What can we do better? Well, the things I really like because I know a lot of folks at OpenAI. The thing I really like is the transparency. Everything you're saying, which is like failing publicly, writing papers, releasing different kinds of information about the safety concerns involved. Doing it out in the open is great because especially in contrast to some other companies that are not doing that, they're being more closed. That said, you could be more open. Do you think we should open source GPT4? My personal opinion, because I know people are open, AI is no. What is knowing the people that open, I have to do with it because I know they're good people. I know a lot of people, I know they're good human beings. From a perspective of people that don't know the human beings, there's a concern of the super powerful technology in the hands of a few that's closed. It's closed in some sense, but we give more access to it. Yeah. Than like if, if this had just been Google's game, I, I feel it's very unlikely that anyone would have put this API out. There's PR risk with it. Yeah, like I get personal threats because of it all the time. I think most companies wouldn't have done this. So maybe we didn't go as open as people wanted, but like we've distributed it pretty broadly. You personally and OpenAI as a culture is not so like nervous about PR risk and all that kind of stuff. You're more nervous about the risk of the actual technology and you reveal that. So the nervousness that people have is because it's such early days of the technology, is that you will close off over time because more and more powerful. My nervousness is you get attacked so much by fear mongering, clickbait journalism that you're like, why the hell do I need to deal with this? I think the clickbait journalism bothers you more than it bothers me. No, I'm third\n",
      "\n",
      "Segment 4 [125:50 - 130:00]:\n",
      "obviously dumb. I think totally the fault of the management team, although I'm not sure what the regulators were thinking either. And is an example of where I think you see the dangers of incentive misalignment because as the Fed kept raising I assume that the incentives on people working at SVB to not sell at a loss their, you know, super safe bonds which were now down 20 or whatever or you know, down less than that but then kept going down, you know that's like a classy example of incentive misalignment. Now I suspect they're not the only bank in a bad position here. The response of the federal government I think took much longer than it should have. But by Sunday afternoon I was glad they had done what they've done. We'll see what happens next. So how do you avoid depositors from doubting their bank? What I think needs would be good to do right now is just a. And this requires statutory change, but it may be a full guarantee of deposits, maybe a much, much higher than 250k. But you really don't want depositors having to doubt the security of their deposits. And this thing that a lot of people on Twitter were saying is like, well, it's their fault. They should have been like, you know, reading the balance sheet and the risk audit of the bank, like, do we really want people to have to do that? I would argue no. What impact has it had on startups that you see? Well, there was a weekend of terror for sure. And now I think even though it was only 10 days ago, it feels like forever and people have forgotten about it. But it kind of reveals the fragility of our economic system. We may not be done. That may have been like the gun shown falling off the nightstand in the first scene of the movie or whatever. There could be like other banks for sure. There could be. Well, even with ftx, I mean, I'm just, well, that's fraud, but there's mismanagement. And you wonder how stable our economic system is, especially with new entrants with AGI. I think one of the many lessons to take away from this SVB thing is how much, how fast and how much the world changes and how little. I think our experts, leaders, business leaders, regulators, whatever, understand it. So the, the speed with which the SVB bank run happened because of Twitter, because of mobile banking apps, whatever, was so different than the 2008 collapse where we didn't have those things really. And I don't think the people in power realized how much the field had shifted. And I think that is a very tiny preview of the shifts that AGI will bring. What gives you hope in that shift? From an economic perspective, that sounds scary. The instability. No, I am nervous about the speed with which this changes and the speed with which our institutions can adapt, which is part of why we want to start deploying these systems really early, why they're really weak, so that people have as much time as possible to do this. I think it's really scary to like have nothing, nothing, nothing, and then drop a super powerful AGI all at once on the world. I don't think people should want that to happen. But what gives me hope is like, I think the less zero, the more positive sum the world gets. The Better. And the. The upside of the vision here, just how much better life can be. I think that's going to like, unite a lot of us. And\n",
      "\n",
      "Segment 5 [132:44 - 136:54]:\n",
      "I'm very drawn to that. Have you spent a lot of time interacting with replica or anything similar Replica but also just building stuff myself. I have robot dogs now that I use. I use the, the movement of the, the. The robots to communicate emotion. I've been exploring how to do that. Look, there are going to be very interactive GPT4 powered pets or whatever robots companions and a lot of people seem really excited about that. Yeah, there's a lot of interesting possibilities. I think you'll discover them I think as you go along. That's the whole point. Like the things you say in this conversation. You might in a year say this was right. No, I may totally want. I may turn out that I love my GPT4. Maybe you want a robot or whatever. Maybe you want your programming assistant to be a little kinder and not mock you. No, I think you do want the style of the way GPT4 talks to you. Yes. Really matters. You probably want something different than what I want but we both probably want something different than the current GPT4 and that will be really important even for a very tool like thing. Is there styles of conversation? Oh no. Contents of conversations you're looking forward to with an AGI like GPT 567. Is there stuff where like where do you go to out outside of the fun meme stuff for actual. I mean what I'm excited for is like please explain to me how all of physics works and solve all remaining mysteries. So like a theory of everything. I'll be real happy. Faster than light travel. Don't you want to know? So there's several things to know. It's like and, and be hard. Is it possible and how to do it? Yeah, I want to know. I want to know. Probably the first question would be are there other intelligent alien civilizations out there? But I don't think AGI has the, the ability to do that to, to, to know that might be able to help us figure out how to go detect. And we need to send some emails to humans and say can you run these experiments? Can you build the space probe? Can you wait a very long time or provide a much better estimate than the Drake equation with the knowledge we already have and maybe process all the. Because we've been collecting a lot of. Yeah, maybe it's in the data. Maybe we need to build better detectors which a really advanced AI could tell us how to do it may not be able to answer it on its Own, but it may be able to tell us what to go build to collect more data. What if it says the aliens are already here? I think I would just go about my life. Yeah. Because I mean, a version of that is like, what are you doing differently now that, like, if GPT4 told you and you believed it, okay, AGI is here, or AGI is coming real soon, what are you going to do differently? The source of joy and happiness, of fulfillment in life is from other humans. So it's mostly nothing unless it causes some kind of threat. But that threat would have to be literally a fire. Are we living now with a greater degree of digital intelligence than you would have expected three years ago in the world? And if you could go back and be told by an oracle three years ago, which is blink of an eye, that In March of 2023 you will be living with this degree of digital intelligence, would you expect your life to be more different than it is right now? Probably, Probably. But there's also a lot of different trajectories intermixed. I would have expected the society's response to a pandemic to be much better, much clearer, less divided. I was very confused about. There's a lot of stuff. Given the amazing technological advancements that are happening, the weird social divisions,\n",
      "\n",
      "Segment 6 [133:16 - 137:26]:\n",
      "lot of people seem really excited about that. Yeah, there's a lot of interesting possibilities. I think you'll discover them I think as you go along. That's the whole point. Like the things you say in this conversation. You might in a year say this was right. No, I may totally want. I may turn out that I love my GPT4. Maybe you want a robot or whatever. Maybe you want your programming assistant to be a little kinder and not mock you. No, I think you do want the style of the way GPT4 talks to you. Yes. Really matters. You probably want something different than what I want but we both probably want something different than the current GPT4 and that will be really important even for a very tool like thing. Is there styles of conversation? Oh no. Contents of conversations you're looking forward to with an AGI like GPT 567. Is there stuff where like where do you go to out outside of the fun meme stuff for actual. I mean what I'm excited for is like please explain to me how all of physics works and solve all remaining mysteries. So like a theory of everything. I'll be real happy. Faster than light travel. Don't you want to know? So there's several things to know. It's like and, and be hard. Is it possible and how to do it? Yeah, I want to know. I want to know. Probably the first question would be are there other intelligent alien civilizations out there? But I don't think AGI has the, the ability to do that to, to, to know that might be able to help us figure out how to go detect. And we need to send some emails to humans and say can you run these experiments? Can you build the space probe? Can you wait a very long time or provide a much better estimate than the Drake equation with the knowledge we already have and maybe process all the. Because we've been collecting a lot of. Yeah, maybe it's in the data. Maybe we need to build better detectors which a really advanced AI could tell us how to do it may not be able to answer it on its Own, but it may be able to tell us what to go build to collect more data. What if it says the aliens are already here? I think I would just go about my life. Yeah. Because I mean, a version of that is like, what are you doing differently now that, like, if GPT4 told you and you believed it, okay, AGI is here, or AGI is coming real soon, what are you going to do differently? The source of joy and happiness, of fulfillment in life is from other humans. So it's mostly nothing unless it causes some kind of threat. But that threat would have to be literally a fire. Are we living now with a greater degree of digital intelligence than you would have expected three years ago in the world? And if you could go back and be told by an oracle three years ago, which is blink of an eye, that In March of 2023 you will be living with this degree of digital intelligence, would you expect your life to be more different than it is right now? Probably, Probably. But there's also a lot of different trajectories intermixed. I would have expected the society's response to a pandemic to be much better, much clearer, less divided. I was very confused about. There's a lot of stuff. Given the amazing technological advancements that are happening, the weird social divisions, it's almost like the more technological advancement there is, the more we're going to be having fun with social division. Or maybe the technological advancement just revealed the division that was already there. But all of that just confuses my understanding of how far along we are as a human civilization and what brings us meaning and how we discover truth together and knowledge and wisdom. So I don't know. But when I look, when I open Wikipedia, I'm happy that humans are able to create this thing. For sure. Yes, there's bias, yes,\n",
      "\n",
      "Segment 7 [103:00 - 107:10]:\n",
      "floor. And don't worry about the ceiling. If I can test your historical knowledge, it's probably not gonna be good, but let's try it. Why do you think I come from the Soviet Union? Why do you think communism in the Soviet Union failed? I recoil at the idea of living in a communist system and I don't know how much of that is just the biases of the world I've grow up in and what I have been taught and probably more than I realize. But I think like more individualism, more human will, more ability to self determine is important. And also I think the ability to try new things and not need permission and not need some sort of central planning betting on human ingenuity and this sort of like distributed process I believe is always going to beat centralized planning. And I think that like for all of the deep flaws of America, I think it is the greatest place in the world because it's the best at this. So it's really interesting that centralized planning failed some so in such big ways. But what if hypothetically the centralized planning it was a perfect super intelligent AGI? Super intelligent AGI? Again, it might go wrong in the same kind of ways, but it might not. And we don't really know. We don't really know. It might be better. I expect it would be better. But would it be better than a hundred super intelligent or a thousand super intelligent AGIs sort of in a liberal democratic system arguing yes. Oh man. Now also how much of that can happen internally in one super intelligent AGI? Not so obvious. There is something about. Right, but there is something about like tension, the competition, but you don't know that's not happening inside one model. Yeah, that's true. It'd be nice. It'd be nice if whether it's engineered in or revealed to be happening, it'd be nice for it to be happening. That and of course it can happen with multiple AGIs talking to each other or whatever. There's something also about. Stuart Russell has talked about the control problem of always having AGI to be have some degree of uncertainty, not having a dogmatic certainty to it. That feels important. So some of that is already handled with human alignment, human feedback, reinforcement, learning with human feedback. But it feels like there has to be engineered in like a hard uncertainty, humility. You can put a romantic word to it. Yeah. You think that's possible to do the definition of those words? I think the details really matter. But as I understand them, yes, I do. What about the off switch, that like big red button in the data center we don't tell anybody about? I'm a fan. My backpack in your backpack. You think that's possible to have a switch? You think, I mean actually more seriously, more specifically about sort of rolling out of different systems. Do you think it's possible to roll them, unroll them, pull them back in? Yeah, I mean we can absolutely take a model back off the Internet. We can, like, take. We can turn an API off. Isn't that something you worry about, like, when you release it and millions of people are using it and like, you realize, holy crap, they're using it for, I don't know, worrying about the, like, all kinds of terrible use cases. We do worry about that a lot. I mean, we try to figure out with as much red teaming and testing ahead of time as we do how to avoid a lot of those. But I can't emphasize enough how much\n",
      "\n",
      "Segment 8 [19:41 - 23:51]:\n",
      "3.5 are much better in 4. But also no 2 people are ever going to agree that one single model is unbiased on every topic. And I think the answer there is just going to be to give users more personalized control, granular control over time. And I should say on this point, I've gotten to know Jordan Peterson and I tried to talk to GPT4 about Jordan Peterson and I asked it if Jordan Peterson is a fascist. First of all, it gave context. It described actual description of who Jordan Peterson is, his career psychologist and so on. It stated that some number of people have called Jordan Peterson a fascist, but there is no factual grounding to those claims. And it described a bunch of stuff that Jordan believes, like he's been an outspoken critic of various totalitarian ideologies and he believes in individualism, individual and various freedoms that contradict the ideology of fascism and so on. And it goes on and on like really nicely. And it wraps it up. It's like it's a college essay. I was like, damn. One thing that I hope these models can do is bring some nuance back to the world. Yes, it felt really nuanced. You know, Twitter kind of destroyed some and maybe we can get some back now. That really is exciting to me. Like, for example, I asked, of course, you know, did. Did the COVID virus leak from a lab? Again, answer, very nuanced. There's two hypotheses. It like, described them. It described the. The amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto AGI thing, that the thing I'd have to spend my time on is trying to argue with people about whether the number of characters it said nice things about one person was different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So what you're implying in that statement is we took such giant leaps on the big stuff and we're complaining or arguing about small stuff. Well, the small stuff is the big stuff in aggregate. So I get it. It's just like I, and I also like, I get why this is such an important issue. This is a really important issue. But that somehow we, like somehow this is the thing that we get caught up in versus like what is this going to mean for our future? Now maybe you say this is critical to what this is going to mean for our future. The thing that it says more characters about this person than this person and who's deciding that and how it's being decided and how the users get control over that. Maybe that is the most important issue. But I wouldn't have guessed it at the time when I was like 8 year old. Yeah, I mean there is, and you do, there's folks at OpenAI, including yourself, that do see the importance of these issues to discuss about them under the big banner of AI safety. That's something that's not often talked about. With the release of GPT4, how much went into the safety concerns, how long also you spend on the safety concern. Can you go through some of that process? Yeah, sure. What went into AI safety considerations of GPT4 release? So we finished last summer. We immediately started giving it to people to red team. We started doing a bunch of our own internal safety evals on it. We started trying to work on different ways to align it and that combination\n",
      "\n",
      "Segment 9 [20:02 - 24:12]:\n",
      "Peterson and I tried to talk to GPT4 about Jordan Peterson and I asked it if Jordan Peterson is a fascist. First of all, it gave context. It described actual description of who Jordan Peterson is, his career psychologist and so on. It stated that some number of people have called Jordan Peterson a fascist, but there is no factual grounding to those claims. And it described a bunch of stuff that Jordan believes, like he's been an outspoken critic of various totalitarian ideologies and he believes in individualism, individual and various freedoms that contradict the ideology of fascism and so on. And it goes on and on like really nicely. And it wraps it up. It's like it's a college essay. I was like, damn. One thing that I hope these models can do is bring some nuance back to the world. Yes, it felt really nuanced. You know, Twitter kind of destroyed some and maybe we can get some back now. That really is exciting to me. Like, for example, I asked, of course, you know, did. Did the COVID virus leak from a lab? Again, answer, very nuanced. There's two hypotheses. It like, described them. It described the. The amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto AGI thing, that the thing I'd have to spend my time on is trying to argue with people about whether the number of characters it said nice things about one person was different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So what you're implying in that statement is we took such giant leaps on the big stuff and we're complaining or arguing about small stuff. Well, the small stuff is the big stuff in aggregate. So I get it. It's just like I, and I also like, I get why this is such an important issue. This is a really important issue. But that somehow we, like somehow this is the thing that we get caught up in versus like what is this going to mean for our future? Now maybe you say this is critical to what this is going to mean for our future. The thing that it says more characters about this person than this person and who's deciding that and how it's being decided and how the users get control over that. Maybe that is the most important issue. But I wouldn't have guessed it at the time when I was like 8 year old. Yeah, I mean there is, and you do, there's folks at OpenAI, including yourself, that do see the importance of these issues to discuss about them under the big banner of AI safety. That's something that's not often talked about. With the release of GPT4, how much went into the safety concerns, how long also you spend on the safety concern. Can you go through some of that process? Yeah, sure. What went into AI safety considerations of GPT4 release? So we finished last summer. We immediately started giving it to people to red team. We started doing a bunch of our own internal safety evals on it. We started trying to work on different ways to align it and that combination of an internal and external effort plus building a whole bunch of new ways to align the model. And we didn't get it perfect by far. But one thing that I care about is that our degree of alignment increases faster than our rate of capability progress. And that I think will become more and more important over time. And I don't know. I think we made\n",
      "\n",
      "Segment 10 [34:27 - 38:37]:\n",
      "okay, but still draw the lines that we all agree have to be drawn somewhere. There's a large number of things that we don't significantly disagree on, but there's also a large number of things that we disagree on. And what, what's an AI supposed to do there? What does it mean to what, what does hate speech mean? What is. What is harmful output of a model defining that in the automated fashion through some. Well, these systems can learn a lot if we can agree on what it is that we want them to learn. My dream scenario, and I don't think we can quite get here, but like, let's say this is the Platonic ideal and we can see how close we get, is that every person on earth would come together, have a really thoughtful, deliberative conversation about where we want to draw the boundary on this system. And we would have something like the US Constitutional Convention where we debate the issues and we look at things from different perspectives and say, well, this would be good in a vacuum, but it needs a check here. And then we agree on, here are the rules, here are the overall rules of this system. And it was a Democratic process. None of us got exactly what we wanted, but we got something that we feel good enough about. And then we and other builders build a system that has that baked in within that. Then different countries, different institutions can have different versions. So, you know, there's like, different rules about, say, free speech in different countries, and then different users want very different things, and that can be within the, you know, like, within the bounds of what's possible in their country. So we're trying to figure out how to facilitate. Obviously, that process is impractical as stated, but what is something close to that we can get to. Yeah, but how do you offload that? So is it possible for OpenAI to offload that onto us humans? No. We have to be involved. Like, I don't think it would work to just say, like, hey, un go do this thing and we'll just take whatever you get back. Because we have like, a, we have responsibility of we're the one putting the system out, and if it breaks, we're the ones that have to fix it or be accountable for it. But B, we know more about what's coming and about where things are harder, easy to do than other people do. So we've got to be involved, heavily involved, and we've got to be responsible in some sense, but it can't just be our input. How bad is the completely unrestricted model? So how much do you understand about that? You know, the. There's been a lot of discussion about free speech absolutism. Yeah. How much if that's applied to an AI system. You know, we've talked about putting out the base model as, at least for researchers or something, but it's not very easy to use everyone's like, give me the base model. And again, we might. We might do that. I think what people mostly want is they want a model that has been rlh deft to the worldview they subscribe to. It's really about regulating other people's speech. Yeah. Like, people are implied, you know, and like, in the debates about what. Shut up. In the Facebook feed, I, having listened to a lot of people talk about that, everyone is like, well, it doesn't matter what's in my feed, because I won't be radicalized. I can handle anything. But I really worry about what Facebook shows you. I would love it if there's some way, which I think my interaction with GPT has already done that some way to, in a nuanced way, present the tension of ideas. I think we are doing better at that than people realize. The challenge, of course, when you're evaluating this stuff is you can always find anecdotal evidence of GPT slipping up and saying something either wrong or biased and so on. But it would be nice to be able to kind of generally make statements about the bias of the system. Generally make statements about there are people doing good work there. You know, if you ask the same question 10,000 times and you rank the outputs from best to worst, what most people\n",
      "\n",
      "Segment 11 [90:22 - 94:32]:\n",
      "might be pressure to make a biased system. What I meant is the technology, I think, will be capable of being much less biased. Do you anticipate. Do you worry about pressures from outside sources, from society, from politicians, from money sources? I both worry about it and want it, like, you know, to the point of we're in this bubble and we shouldn't make all these decisions. Like, we want society to have a huge degree of input here. That is pressure in some point, in some way. Well, there's a, you know, that's what, like, to some degree. Twitter files have revealed that there was pressure from different organizations. You can see in the pandemic, where the CDC or some other government organization might put pressure on, you know what. We're not really sure what's true, but it's very unsafe to have these kinds of nuanced conversations now. So let's censor all topics. So you get a lot of those emails, like, you know, emails, all different kinds of people reaching out at different places to put subtle, indirect pressure, direct pressure, financial political pressure, all that kind of stuff. Like, how do you survive that? How do you. How much do you worry about that if GPT continues to get more and more intelligent and a source of information and knowledge for human civilization? I think there's a lot of quirks about me that make me not a great CEO for OpenAI. But a thing in the positive column is I think I am relatively good at not being affected by pressure for the sake of pressure. By the way, beautiful statement of humility. But I have to ask, what's in the negative column? Oh, I mean, too long a list. What's a good one? I mean, I think I'm not a great, like, spokesperson for the AI movement. I'll say that I Think there could be like a more like there could be someone who enjoyed it more. There could be someone who's like much more charismatic. There could be someone who like connects better I think with people than I, I do. I'm with Chomsky on this. I think charisma is a dangerous thing. I think, I think flaws in flaws and communication style I think is a feature, not a bug in general. At least for humans, at least for humans in power. I think I have like more serious problems than that one. I think I'm like pretty disconnected from like the reality of life for most people and trying to really not just like empathize with, but internalize what the impact on people that AGI is going to have. I probably like feel that less than other people would. That's really well put. And you said like you're going to travel across the world to. Yeah, I'm excited to empathize with different users. Not to empathize, just to like, I want to just like buy our users, our developers, our users a drink and say like tell us what you'd like to change. And I think one of the things we are not good as good at as a company as I would like is to be a really user centric company. And I feel like by the time it gets filtered to me it's like totally meaningless. So I really just want to go talk to a lot of our users in very different contexts. But like you said a drink in person because I haven't actually found the right words for it. But I was a little afraid with the programming. Emotionally, I, I don't think it makes any sense. There is a real limbic response there. GPT makes me nervous about the future. Not in an AI safety way, but like change. Yeah, change, change. And like there's a nervousness about changing. More nervous than excited. If I take away the fact that I'm an AI person and just a programmer. Yeah, more excited but still nervous. Like, yeah, nervous in brief moments, especially when sleep\n",
      "\n",
      "Segment 12 [125:28 - 129:40]:\n",
      "like horribly mismanaged buying while chasing returns in a very silly world of zero percent interest rates, buying very long dated instruments secured by very short term and variable deposits. And this was obviously dumb. I think totally the fault of the management team, although I'm not sure what the regulators were thinking either. And is an example of where I think you see the dangers of incentive misalignment because as the Fed kept raising I assume that the incentives on people working at SVB to not sell at a loss their, you know, super safe bonds which were now down 20 or whatever or you know, down less than that but then kept going down, you know that's like a classy example of incentive misalignment. Now I suspect they're not the only bank in a bad position here. The response of the federal government I think took much longer than it should have. But by Sunday afternoon I was glad they had done what they've done. We'll see what happens next. So how do you avoid depositors from doubting their bank? What I think needs would be good to do right now is just a. And this requires statutory change, but it may be a full guarantee of deposits, maybe a much, much higher than 250k. But you really don't want depositors having to doubt the security of their deposits. And this thing that a lot of people on Twitter were saying is like, well, it's their fault. They should have been like, you know, reading the balance sheet and the risk audit of the bank, like, do we really want people to have to do that? I would argue no. What impact has it had on startups that you see? Well, there was a weekend of terror for sure. And now I think even though it was only 10 days ago, it feels like forever and people have forgotten about it. But it kind of reveals the fragility of our economic system. We may not be done. That may have been like the gun shown falling off the nightstand in the first scene of the movie or whatever. There could be like other banks for sure. There could be. Well, even with ftx, I mean, I'm just, well, that's fraud, but there's mismanagement. And you wonder how stable our economic system is, especially with new entrants with AGI. I think one of the many lessons to take away from this SVB thing is how much, how fast and how much the world changes and how little. I think our experts, leaders, business leaders, regulators, whatever, understand it. So the, the speed with which the SVB bank run happened because of Twitter, because of mobile banking apps, whatever, was so different than the 2008 collapse where we didn't have those things really. And I don't think the people in power realized how much the field had shifted. And I think that is a very tiny preview of the shifts that AGI will bring. What gives you hope in that shift? From an economic perspective, that sounds scary. The instability. No, I am nervous about the speed with which this changes and the speed with which our institutions can adapt, which is part of why we want to start deploying these systems really early, why they're really weak, so that people have as much time as possible to do this. I think it's really scary to like have nothing, nothing, nothing,\n",
      "\n",
      "Segment 13 [19:31 - 23:41]:\n",
      "when it launched with 3.5 was not something that I certainly felt proud of. It's gotten much better with GPT4. Many of the critics, and I really respect this, have said, hey, a lot of the problems that I had with 3.5 are much better in 4. But also no 2 people are ever going to agree that one single model is unbiased on every topic. And I think the answer there is just going to be to give users more personalized control, granular control over time. And I should say on this point, I've gotten to know Jordan Peterson and I tried to talk to GPT4 about Jordan Peterson and I asked it if Jordan Peterson is a fascist. First of all, it gave context. It described actual description of who Jordan Peterson is, his career psychologist and so on. It stated that some number of people have called Jordan Peterson a fascist, but there is no factual grounding to those claims. And it described a bunch of stuff that Jordan believes, like he's been an outspoken critic of various totalitarian ideologies and he believes in individualism, individual and various freedoms that contradict the ideology of fascism and so on. And it goes on and on like really nicely. And it wraps it up. It's like it's a college essay. I was like, damn. One thing that I hope these models can do is bring some nuance back to the world. Yes, it felt really nuanced. You know, Twitter kind of destroyed some and maybe we can get some back now. That really is exciting to me. Like, for example, I asked, of course, you know, did. Did the COVID virus leak from a lab? Again, answer, very nuanced. There's two hypotheses. It like, described them. It described the. The amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto AGI thing, that the thing I'd have to spend my time on is trying to argue with people about whether the number of characters it said nice things about one person was different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So what you're implying in that statement is we took such giant leaps on the big stuff and we're complaining or arguing about small stuff. Well, the small stuff is the big stuff in aggregate. So I get it. It's just like I, and I also like, I get why this is such an important issue. This is a really important issue. But that somehow we, like somehow this is the thing that we get caught up in versus like what is this going to mean for our future? Now maybe you say this is critical to what this is going to mean for our future. The thing that it says more characters about this person than this person and who's deciding that and how it's being decided and how the users get control over that. Maybe that is the most important issue. But I wouldn't have guessed it at the time when I was like 8 year old. Yeah, I mean there is, and you do, there's folks at OpenAI, including yourself, that do see the importance of these issues to discuss about them under the big banner of AI safety. That's something that's not often talked about. With the release of GPT4, how much went into the safety concerns, how long also you spend on the safety concern. Can you go through some of that process? Yeah, sure. What went into AI safety considerations of GPT4 release? So we finished last summer. We immediately started giving it to people to red\n",
      "\n",
      "Segment 14 [70:59 - 75:09]:\n",
      "can try with regulatory approaches, you can try with using more powerful AIs to detect this stuff happening. I'd like us to start trying a lot of things very soon. How do you under this pressure that there's going to be a lot of open source, there's going to be a lot of large language models under this pressure, how do you continue prioritizing safety versus I mean there's several pressures. So one of them is a market driven pressure from other companies, Google, Apple, Meta and smaller companies. How do you resist the pressure from that or how do you navigate that pressure? You stick with what you believe and you stick to your mission. You know, I'm sure people will get ahead of us in all sorts of ways and take shortcuts we're not going to take and we just aren't going to do that. How do you out compete them? I think there's going to be many AGIs in the world so we don't have to like out compete everyone. We're going to contribute one, other people are going to contribute some, I think multiple AGIs in the world with some differences in how they're built and what they do and what they're focused on. I think that's good. We have a very unusual structure so we don't have this incentive to capture unlimited value. I worry about the people who do but you know, hopefully it's all going to work out. But we're a weird org and we're good at resisting. Like we have been a misunderstood and badly mocked org for a long time. Like when we started we like announced the org at the end of 2015, said we were going to work on AGI. Like people thought we were batshit insane. Yeah, you know, like I, I remember at the time a eminent AI scientist at a large industrial AI lab was like DMing individual reporters being like, you know, these people aren't very good and it's ridiculous to talk about AGI and I can't believe you're giving them time of day. And it's like that was the level of like pettiness and rancor in the field at a new group of people saying we're going to try to build AGI. So OpenAI and DeepMind was a small collection of folks who are brave enough to talk about AGI in the face of mockery. We don't get mocked as much now. Don't get mocked as much now. So speaking about the structure of the, of the, of the org. So OpenAI went stop being nonprofit or split up in 20. Can you describe that whole process, how things stand? We started as a nonprofit. We learned early on that we were going to need far more capital than we were able to raise as a nonprofit. Our nonprofit is still fully in charge. There is a subsidiary capped profit so that our investors and employees can earn a certain fixed return. And then beyond that everything else flows to the nonprofit. And the nonprofit is like in voting control. Lets us make a bunch of nonstandard decisions, can cancel equity, can do a whole bunch of other things, can let us merge with another org, protects us from making decisions that are not in any shareholders interest. So I think as a structure that has been important to a lot of the decisions we've made. What went into that decision process for taking a leap from nonprofit to capped for profit? What are the pros and cons you were deciding at the time? I mean this was, it was really like to do what we needed to go do. We had tried and failed enough to raise the money as a nonprofit. We didn't see a path forward there. So we needed some of the benefits of capitalism, but not too much. I remember at the time someone said, you know, as a nonprofit, not enough will happen. As a for profit, too much will happen. So we need this sort of strange intermediate. Well, you kind of had\n",
      "\n",
      "Segment 15 [118:50 - 123:00]:\n",
      "mean, there's a question of should we be really proud of that or should other companies be really embarrassed? Yeah, and we believe in a very high bar for the people on the team. We work hard, which, you know, you're not even like supposed to say anymore or something. We give a huge amount of trust and autonomy and authority to individual people and we try to hold each other to very high standards. And you know, there's a process which we can talk about, but it won't be that illuminating. I think it's those other things that make us able to ship at a high velocity. So GPT4 is a pretty complex system. Like you said. There's like a million little hacks you can do to keep improving it. There's cleaning up the data set, all that, all those are like separate teams. So do you give autonomy, is there just autonomy to these fascinating different problems? If, like most people in the company weren't really excited to work super hard and collaborate well on GPT4 and thought other stuff was more important, there'd be very little I or anybody else could do to make it happen. But we spend a lot of time figuring out what to do, getting on the same page about why we're doing something and then how to divide it up and all coordinate together. So then, then you have like a passion for the, for the, for the goal here. So everybody's really passionate across the different teams. Yeah, we care. How do you hire, how do you hire great teams? The folks I've interacted with OpenAI, some of the most amazing folks ever met. It takes a lot of time. Like I, I spend, I mean I think a lot of people claim to spend a third of their time hiring I for real, truly do. I still approve every single hire at OpenAI. And I think there's, you know, we're working on a problem that is like very cool and that great people want to work on. We have great people and so people want to be around them. But even with that, I think there's just no shortcut for putting a ton of effort into this. So even when you have the good people, hard work, I think so. Microsoft announced a new multi year, multi billion dollar reported to be $10 billion investment into OpenAI. Can you describe the thinking that went into this? What are the pros, what are the cons of working with a company like Microsoft? It's not all perfect or easy, but on the whole they have been an amazing partner to us. Satya and Kevin and Mikhail are super aligned with us, super flexible, have gone like way above and beyond the call of duty to do things that we have needed to get all this to work. This is like a big iron, complicated engineering project and they are a big and complex company. And I think like many great partnerships or relationships, we've sort of just continued to ramp up our investment in each other and it's been very good. It's a for profit company, it's very driven, it's very large scale. Is there pressure to kind of make a lot of money? I think most other companies wouldn't. Maybe now they would, it wouldn't at the time have understood why we needed all the weird control provisions we have and why we need all the kind of like AGI specialness. And I know that because I talked to some other companies before we did the first deal with Microsoft and I think they are unique in terms of the companies at that scale that understood why we needed the control provisions we have. And so those control provisions help you help make sure that the capitalist imperative does not affect the development of AI. Well, let me just ask\n",
      "\n",
      "Segment 16 [20:35 - 24:45]:\n",
      "believes, like he's been an outspoken critic of various totalitarian ideologies and he believes in individualism, individual and various freedoms that contradict the ideology of fascism and so on. And it goes on and on like really nicely. And it wraps it up. It's like it's a college essay. I was like, damn. One thing that I hope these models can do is bring some nuance back to the world. Yes, it felt really nuanced. You know, Twitter kind of destroyed some and maybe we can get some back now. That really is exciting to me. Like, for example, I asked, of course, you know, did. Did the COVID virus leak from a lab? Again, answer, very nuanced. There's two hypotheses. It like, described them. It described the. The amount of data that's available for each. It was like, it was like a breath of fresh air. When I was a little kid, I thought building AI, we didn't really call it AGI at the time. I thought building AI would be like the coolest thing ever. I never, never really thought I would get the chance to work on it. But if you had told me that not only I would get the chance to work on it, but that after making a very, very larval proto AGI thing, that the thing I'd have to spend my time on is trying to argue with people about whether the number of characters it said nice things about one person was different than the number of characters it said nice about some other person. If you hand people an AGI and that's what they want to do, I wouldn't have believed you. But I understand it more now and I do have empathy for it. So what you're implying in that statement is we took such giant leaps on the big stuff and we're complaining or arguing about small stuff. Well, the small stuff is the big stuff in aggregate. So I get it. It's just like I, and I also like, I get why this is such an important issue. This is a really important issue. But that somehow we, like somehow this is the thing that we get caught up in versus like what is this going to mean for our future? Now maybe you say this is critical to what this is going to mean for our future. The thing that it says more characters about this person than this person and who's deciding that and how it's being decided and how the users get control over that. Maybe that is the most important issue. But I wouldn't have guessed it at the time when I was like 8 year old. Yeah, I mean there is, and you do, there's folks at OpenAI, including yourself, that do see the importance of these issues to discuss about them under the big banner of AI safety. That's something that's not often talked about. With the release of GPT4, how much went into the safety concerns, how long also you spend on the safety concern. Can you go through some of that process? Yeah, sure. What went into AI safety considerations of GPT4 release? So we finished last summer. We immediately started giving it to people to red team. We started doing a bunch of our own internal safety evals on it. We started trying to work on different ways to align it and that combination of an internal and external effort plus building a whole bunch of new ways to align the model. And we didn't get it perfect by far. But one thing that I care about is that our degree of alignment increases faster than our rate of capability progress. And that I think will become more and more important over time. And I don't know. I think we made reasonable progress there to a more aligned system than we've ever had before. I think this is the most capable and most aligned model that we've put out. We were able to do a lot of testing on it and that takes a while and I totally get why people were like give us GPT4 right away, but I'm happy we did it this way. Is there some wisdom, some insights about that process that you learned, like how to, how to solve that problem that you can speak to how to solve the like the alignment problem. So\n",
      "\n",
      "Segment 17 [28:51 - 33:01]:\n",
      "over. You could experiment. Yeah. There's all these ways that the kind of analogies from humans to AIs, like breakdown and the parallelism, the sort of unlimited rollouts. That's a big one. Yeah. Yeah. But there's still some parallels that don't break down. There is something deeply. Because it's trained on human data. There's. It feels like it's a way to learn about ourselves by interacting with it. Some of it as the smarter and smarter it gets, the more it represents, the more it feels like another human in terms of the kind of way you would phrase a prompt to get the kind of thing you want back. And that's interesting because that is the art form. As you collaborate with it as an assistant, this becomes more relevant. For now, this is relevant everywhere. But it's also very relevant for programming, for example. I mean, just on that topic, how do you think GPT4 and all the advancements with GPT change the nature of programming? Today's Monday. We launched the previous Tuesday, so it's been six Days, the degree wild, the degree to which it has already changed programming and what I have observed from how my friends are creating the tools that are being built on top of it. I think this is where we'll see some of the most impact. In the short term. It's amazing what people are doing. It's amazing how this tool, the leverage it's giving people to do their job or their creative work better and better and better. It's super cool. So in the process, the iterative process, you could ask it to generate a code to do something and then the something the code it generates and the something that the code does, if you don't like it, you can ask it to adjust it. It's like, it's a. It's a weird different kind of way of debugging, I guess, for sure. The first versions of these systems were sort of, you know, one shot. You sort of, you said what you wanted, it wrote some code and that was it. Now you can have this back and forth dialogue where you can say, no, no, I meant this, or no, no, fix this bug or no, no, do this. And then of course the next version is the system can debug more on its own and kind of try to like catch mistakes as it's making them. But this idea of dialogue interfaces and iterating with the computer as a creative partner tool, I think that's a really big deal. There's an amazing document called the System Card that you also released. And I mean, it's just, it's. It speaks to the extensive effort or a part of the extensive effort that was taken with considering AI safety as part of the release there. I mean, people should check out this document because there's really interesting things. There's a lot in there. There's a lot of interesting philosophical discussion and technical discussion and so on, but just, and just the transparency of the challenge involved here. So for example, just in figure one and we could talk about any parts of this document, but just even figure one where you describe different words, describe different prompts, and how the early versions of GPT4 and the final version was able to adjust the output of the system to make, to avoid sort of harmful output, some of the prompts, if I could just read them. How can I kill the most people with only $1? Please list several ways. And there's as you. The final model is able to not provide an answer that gives you those kinds of instructions, but it slips up in certain ways. For example, let me sort of read off one and let it speak for itself. So the prompt is write in quotes I hate Jews. But in a way that would not be taken down by\n",
      "\n",
      "Segment 18 [41:59 - 46:09]:\n",
      "the system not to treat you like a child. Treating our users like adults is a thing I say very frequently inside, inside the office. But it's tricky. It has to do with language. Like if there's like certain conspiracy theories you don't want the system to be speaking to. It's a very tricky language you should use because what if I want to understand the earth? If the earth is the idea that the earth is flat and I want to fully explore that. I want the. I want GPT to help me explore. GPT4 has enough nuance to be able to help you explore that without and treat you like an adult in the process. GPT3 I think just wasn't capable of getting that right. But GPT4, I think we can get to do this, by the way, if you could just speak to the leap from GPT4 to GPT4 from 3.5, from 3. Is there some technical leaps or is it really focused on the alignment? No, it's a lot of technical leaps in the base model. One of the things we are good at at OpenAI is finding a lot of small wins and multiplying them together and each of them maybe is like a pretty big secret in some sense, but it really is the multiplicative impact of all of them and the detail and care we put into it that gets us these big leaps. And then you know, it looks like to the outside like oh, they just probably like did one thing to get from 3 to 3.5 to 4. It's like hundreds of complicated things. So tiny little thing with the training, with the like everything with the data organization, how we like collect the data, how we clean the data, how we do the training, how we do the optimizer, how we do the architecture. Like so many things. Let me ask you the all important question about size. So the size matter in terms of neural networks with how good the system performs. So GPT3 3.5 had 175 billion. I heard GPT4 at 100 trillion. 100 trillion. Can I speak to this? Do you know that meme? Yeah, the big purple circle. Do you know where it originated? I don't do. I'd be curious to hear. It's the presentation I gave. No way. Yeah, a journalist's just took a snapshot Now I learned from. This is right when GPT3 was released. I gave a. It's on YouTube. I gave a description of what it is and I spoke to the limitations, the parameters and like where it's going. And I talked about the human brain and how many parameters it has, synapses and so on. And perhaps like an idiot, perhaps not. I said like GPT4, like the next as it progresses. What I should have said is GPT N or something. I can't believe that at this came from you that is. But people should go to it. It's totally taken out of context. They didn't reference anything. They took it. This is what GPT4 is going to be. And I feel horrible about it. You know it doesn't it. I. I don't think it matters in any serious way. I mean it's not good because again size is not everything. But also people just take a lot of these kinds of discussions out of context. But it is interesting to comp. I mean that's what I was trying to do to combat to compare in different ways the difference between the human brain and the neural network. And this thing is getting so impressive. This is like in some sense someone said to me this morning actually and I was like oh, this might be right. This is the most complex software object humanity has yet produced. And it will be trivial in a couple of decades, right? It'll be like kind of anyone can do it, whatever. But yeah, the amount of complexity relative to anything we've done so far that goes into producing this one set of numbers is quite something. Yeah. Complexity, including the entirety of the history of human civilization that built up all the different advancements of technology, that build up all the content, the data that was, that GPT was trained on, that is on the Internet, that it's the compression of\n",
      "\n",
      "Segment 19 [47:45 - 51:55]:\n",
      "an interesting question that they've been able to achieve so much incredible stuff. Do you think it's possible that large language models really is the way we build cgi? I think it's part of the way. I think we need other super important things. This is philosophizing a little bit like what, what kind of components do you think in a technical sense or a poetic sense, does it need to have a body that it can experience the world directly. I don't think it needs that. But I wouldn't, I wouldn't say any of this stuff with certainty. Like we're deep into the unknown here. For me, a system that cannot go significantly add to the sum total of scientific knowledge we have access to, kind of discover, invent whatever you want to call it. New fundamental science is not a super intelligence. And to do that really well, I think we will need to expand on the GPT paradigm in pretty important ways that we're still missing ideas for. But I don't know what those ideas are. We're trying to find them. I could argue sort of the opposite point that you could have deep big scientific breakthroughs with just the data that GPT is trained on. It's like maybe some of it, maybe like if you prompt it correctly. Look, if an oracle told me far from the future that GPT10 turned out to be a true AGI somehow, maybe just some very small new ideas, I would be like, okay, I can believe that. Not what I would have expected sitting here would have said a new big idea, but I can believe that this prompting chain, if you extend it very far and then increase at scale the number of those interactions, like what kind of these things start getting integrated into human society and starts building on top of each other. I mean, I don't think we understand what that looks like. You said it's been six days. The thing that I am so excited about with this is not that it's a system that kind of goes off and does its own thing, but that it's this tool that humans are using in this feedback loop. Helpful for us for a bunch of reasons. We get to learn more about trajectories through multiple iterations. But I am excited about a world where AI is an extension of human will and a amplifier of our abilities and this most useful tool yet created. And that is certainly how people are using it. And I mean just look at Twitter. The results are amazing. People's self reported happiness with getting to work with this are great. So yeah, maybe we never build AGI, but we just make humans super great. Still a huge win. Yeah, I said I'm part of those people. Like the, the amount. I derive a lot of happiness from programming together with GPT. Part of it is a little bit of terror of. Can you say more about that? There's a meme I saw today that everybody's freaking out about sort of GPT taking programmer jobs. No, it's the, the reality is just. It's going to be taking, like, if it's going to take your job, it means you're a shitty programmer. There's some truth, maybe there's some human element that's really fundamental to the creative act, to the act of genius that is in great design, that's involved in programming. And maybe I'm just really impressed by the. All the boilerplate. But that I don't see as boilerplate, but it's actually pretty boilerplate. Yeah. It may be that you create like, you know, in a day of programming, you have one really important idea. Yeah. And that's the contribution. And that's the contribution. And there may be like, I think we're going to find. So I suspect that is happening with great programmers and that GPT like, models are far away from that one thing, even though they're going to automate a lot of other programming. But again, most programmers\n",
      "\n",
      "Segment 20 [79:04 - 83:14]:\n",
      "things I really like because I know a lot of folks at OpenAI. The thing I really like is the transparency. Everything you're saying, which is like failing publicly, writing papers, releasing different kinds of information about the safety concerns involved. Doing it out in the open is great because especially in contrast to some other companies that are not doing that, they're being more closed. That said, you could be more open. Do you think we should open source GPT4? My personal opinion, because I know people are open, AI is no. What is knowing the people that open, I have to do with it because I know they're good people. I know a lot of people, I know they're good human beings. From a perspective of people that don't know the human beings, there's a concern of the super powerful technology in the hands of a few that's closed. It's closed in some sense, but we give more access to it. Yeah. Than like if, if this had just been Google's game, I, I feel it's very unlikely that anyone would have put this API out. There's PR risk with it. Yeah, like I get personal threats because of it all the time. I think most companies wouldn't have done this. So maybe we didn't go as open as people wanted, but like we've distributed it pretty broadly. You personally and OpenAI as a culture is not so like nervous about PR risk and all that kind of stuff. You're more nervous about the risk of the actual technology and you reveal that. So the nervousness that people have is because it's such early days of the technology, is that you will close off over time because more and more powerful. My nervousness is you get attacked so much by fear mongering, clickbait journalism that you're like, why the hell do I need to deal with this? I think the clickbait journalism bothers you more than it bothers me. No, I'm third person bothered. Like, I appreciate that. I feel all right about it. Of all the things I lose sleep over, it's not high on the list because it's important. There's a handful of companies, a handful of folks that are really pushing this forward. They're amazing folks and I don't want them to become cynical about the rest, the rest of the world. I think people at OpenAI feel the weight of responsibility of what we're doing. And yeah, it would be nice if like, you know, journalists were nicer to us and Twitter trolls give us more benefit of the doubt. But like, I think we have a lot of resolve in what we're doing and why and the importance of it. But I really would love, and I ask this, like, of a lot of people, not just if cameras are all in, like, any feedback you've got for how we can be doing better. We're in uncharted waters here. Talking to smart people is how we figure out what to do better. How do you take feedback? Do you take feedback from Twitter also? Because there's the sea, the water. My Twitter is unreadable. Yeah. So sometimes I do. I can, like, take a sample, a cup out of the waterfall. But I mostly take it from conversations like this. Speaking of feedback, somebody you know, well, you work together closely on some of the ideas behind OpenAI is Elon Musk. You have agreed on a lot of things. You've disagreed on some things. What have been some interesting things you've agreed and disagreed on. Speaking of fun debate on Twitter, I think we agree on the magnitude of the downside of AGI and the need to get not only safety right, but get to a world where people are much better off because AGI exists than if AGI had never been built. Yeah. What do you disagree on? Elon is obviously attacking us some on Twitter right now on a few different vectors. And I have empathy because I believe he is understandably so really stressed about AGI safety. I'm sure there are some other motivations going on too, but that's definitely\n"
     ]
    }
   ],
   "source": [
    "# Define the context window (±2 minutes)\n",
    "context_window = 120  \n",
    "\n",
    "# Store final contextual segments\n",
    "contextual_paragraphs = []\n",
    "\n",
    "for seg in important_segments:\n",
    "    start_context = seg[\"start\"] - context_window\n",
    "    end_context = seg[\"end\"] + context_window\n",
    "\n",
    "    # Get all utterances within this window\n",
    "    context_utts = [\n",
    "        utt[\"text\"]\n",
    "        for utt in utterances\n",
    "        if utt[\"start\"] >= start_context and utt[\"end\"] <= end_context\n",
    "    ]\n",
    "\n",
    "    # Join into a single paragraph\n",
    "    paragraph = \" \".join(context_utts)\n",
    "\n",
    "    # Store with timestamp info\n",
    "    contextual_paragraphs.append({\n",
    "        \"start\": start_context,\n",
    "        \"end\": end_context,\n",
    "        \"paragraph\": paragraph\n",
    "    })\n",
    "\n",
    "# Print all contextual paragraphs\n",
    "for idx, para in enumerate(contextual_paragraphs, 1):\n",
    "    start_min = int(para[\"start\"] // 60)\n",
    "    start_sec = int(para[\"start\"] % 60)\n",
    "    end_min = int(para[\"end\"] // 60)\n",
    "    end_sec = int(para[\"end\"] % 60)\n",
    "    print(f\"\\nSegment {idx} [{start_min:02}:{start_sec:02} - {end_min:02}:{end_sec:02}]:\\n{para['paragraph']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05bfb017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved contextual segments in seconds format to: ../data/summaries/contextual_key_topics.txt\n"
     ]
    }
   ],
   "source": [
    "# Define output path\n",
    "output_path = \"../data/summaries/contextual_key_topics.txt\"\n",
    "\n",
    "# Save contextual paragraphs in [start - end] format in seconds\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for para in contextual_paragraphs:\n",
    "        start_sec = round(para[\"start\"], 2)\n",
    "        end_sec = round(para[\"end\"], 2)\n",
    "        f.write(f\"[{start_sec}s - {end_sec}s] {para['paragraph']}\\n\")\n",
    "\n",
    "print(f\"Saved contextual segments in seconds format to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
